{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detección de caras con webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import FaceNormalizationUtils as faceutils\n",
    "# My face detectors interface\n",
    "import FaceDetectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el bucle de procesamiento, las teclas 'd' y 'e' `permiten respectivamenet cambiar de modelo de detección de caras, y en su caso de máscara de detección del rostro.\n",
    "\n",
    "La ejecución de la siguiente celda produce error al no disponer de los archivos shape_predictor_5_face_landmarks.dat y shape_predictor_68_face_landmarks.dat que por su tamaño no se incluyeron en el repositorio. Pueden descargarse desde el enlace proporcionado en el campus virtual (opción aconsejada), o\n",
    "desde el [repositorio de archivos de dlib](http://dlib.net/files/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normalizatorHS = faceutils.Normalization()\n",
    "\n",
    "# Face detectors interface\n",
    "FDet = FaceDetectors.FaceDetector()\n",
    "\n",
    "# Fonts\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Webcam connection\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Check for other cameras\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print('Camera error')\n",
    "            exit(0)\n",
    "        else:\n",
    "            print('Camera 0')\n",
    "    else:\n",
    "        print('Camera 1')\n",
    "else:\n",
    "    print('Camera 0')\n",
    "\n",
    "    # Face detection and eye model setup\n",
    "imodoF = 0\n",
    "imodoE = 0\n",
    "\n",
    "debug = 0\n",
    "\n",
    "#Set camera resolution\n",
    "cap.set(3,640);\n",
    "cap.set(4,480);\n",
    "\n",
    "while True:\n",
    "    # Get frame\n",
    "    t = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    # For HS normalization\n",
    "    B, G, R = cv2.split(frame)\n",
    "\n",
    "    # Search face with a specific setup for face and eye detection\n",
    "    values = FDet.SingleFaceEyesDetection(frame, FDet.FaceDetectors[imodoF], FDet.EyeDetectors[imodoE])\n",
    "    if values is not None:\n",
    "        face, eyes, shape = values\n",
    "\n",
    "        #draws face container\n",
    "        [x, y , w, h] = face\n",
    "        if x > -1:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "            # draws eyes and mask if available\n",
    "            [lex, ley, rex, rey] = eyes\n",
    "            if lex > -1:\n",
    "                # Show detected facial elements\n",
    "                if imodoF > 0:\n",
    "                    for (x, y) in shape:\n",
    "                        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                        cv2.circle(frame, (x, y), 2, (255, 255, 255), -1)\n",
    "\n",
    "                cv2.circle(frame, ((int)(lex), (int)(ley)), 4, (0, 0, 255), -1)\n",
    "                cv2.circle(frame, ((int)(rex), (int)(rey)), 4, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "                # Normalize and show\n",
    "                # color channels\n",
    "                normalizatorHS.normalize_gray_img(B, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Bnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(G, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Gnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(R, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Rnorm = normalizatorHS.normf_image\n",
    "                NormBGR = cv2.merge((Bnorm, Gnorm, Rnorm))\n",
    "                cv2.imshow(\"Normalized\", NormBGR)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        print(\"Processing time : {:.3f}\".format(time.time() - t))\n",
    "\n",
    "    # Show resulting image\n",
    "    cv2.putText(frame, FDet.FaceDetectors[imodoF], (10, 20), font, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    if imodoF == 1 or imodoF == 2:\n",
    "        cv2.putText(frame, FDet.EyeDetectors[imodoE], (50, 20), font, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Cam', frame)\n",
    "    \n",
    "    # Esc to finish\n",
    "    tec = cv2.waitKey(40)\n",
    "    if tec & tec == 27:  # Esc\n",
    "        break\n",
    "    # Face detector change\n",
    "    elif tec & 0xFF == ord('d'):\n",
    "        imodoF = imodoF + 1\n",
    "        if imodoF >= len(FDet.FaceDetectors):\n",
    "            imodoF = 0\n",
    "    #Eye detector change\n",
    "    elif tec & 0xFF == ord('e'):\n",
    "        imodoE = imodoE + 1\n",
    "        if imodoE >= len(FDet.EyeDetectors):\n",
    "            imodoE = 0\n",
    "\n",
    "# Close windoews and release camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 0\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\guill\\OneDrive\\Documentos\\VC\\VC\\V1_p4\\VC_P4.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/guill/OneDrive/Documentos/VC/VC/V1_p4/VC_P4.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m   \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/guill/OneDrive/Documentos/VC/VC/V1_p4/VC_P4.ipynb#W4sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m faces \u001b[39m=\u001b[39m faceClassif\u001b[39m.\u001b[39mdetectMultiScale(frame,\u001b[39m1.3\u001b[39m,\u001b[39m5\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/guill/OneDrive/Documentos/VC/VC/V1_p4/VC_P4.ipynb#W4sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m values \u001b[39m=\u001b[39m FDet\u001b[39m.\u001b[39;49mSingleFaceEyesDetection(frame, FDet\u001b[39m.\u001b[39;49mFaceDetectors[\u001b[39m3\u001b[39;49m], FDet\u001b[39m.\u001b[39;49mEyeDetectors[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/guill/OneDrive/Documentos/VC/VC/V1_p4/VC_P4.ipynb#W4sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m   \u001b[39m# Detectamos las caras en el fotograma.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/guill/OneDrive/Documentos/VC/VC/V1_p4/VC_P4.ipynb#W4sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guill\\OneDrive\\Documentos\\VC\\VC\\V1_p4\\FaceDetectors.py:354\u001b[0m, in \u001b[0;36mFaceDetector.SingleFaceEyesDetection\u001b[1;34m(self, img, facedet, eyesdet)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m face, eyes, shape\n\u001b[0;32m    353\u001b[0m \u001b[39melif\u001b[39;00m facedet \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMTCNN\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 354\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDetectLargestFaceEyesMTCNN(img)\n\u001b[0;32m    356\u001b[0m     \u001b[39mif\u001b[39;00m values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m         face, eyes, shape \u001b[39m=\u001b[39m values\n",
      "File \u001b[1;32mc:\\Users\\guill\\OneDrive\\Documentos\\VC\\VC\\V1_p4\\FaceDetectors.py:311\u001b[0m, in \u001b[0;36mFaceDetector.DetectLargestFaceEyesMTCNN\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDetectLargestFaceEyesMTCNN\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m--> 311\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetectormtcnn\u001b[39m.\u001b[39;49mdetect_faces(img)\n\u001b[0;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m         index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetLargestMTCNNBB(results)\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\mtcnn\\mtcnn.py:300\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39m# We pipe here each of the stages\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m stages:\n\u001b[1;32m--> 300\u001b[0m     result \u001b[39m=\u001b[39m stage(img, result[\u001b[39m0\u001b[39;49m], result[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    302\u001b[0m [total_boxes, points] \u001b[39m=\u001b[39m result\n\u001b[0;32m    304\u001b[0m bounding_boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\mtcnn\\mtcnn.py:410\u001b[0m, in \u001b[0;36mMTCNN.__stage2\u001b[1;34m(self, img, total_boxes, stage_status)\u001b[0m\n\u001b[0;32m    407\u001b[0m tempimg \u001b[39m=\u001b[39m (tempimg \u001b[39m-\u001b[39m \u001b[39m127.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.0078125\u001b[39m\n\u001b[0;32m    408\u001b[0m tempimg1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(tempimg, (\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m--> 410\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rnet\u001b[39m.\u001b[39;49mpredict(tempimg1)\n\u001b[0;32m    412\u001b[0m out0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(out[\u001b[39m0\u001b[39m])\n\u001b[0;32m    413\u001b[0m out1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(out[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\keras\\engine\\training.py:2379\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2377\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2378\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m-> 2379\u001b[0m     \u001b[39mwith\u001b[39;49;00m data_handler\u001b[39m.\u001b[39;49mcatch_stop_iteration():\n\u001b[0;32m   2380\u001b[0m         \u001b[39mfor\u001b[39;49;00m step \u001b[39min\u001b[39;49;00m data_handler\u001b[39m.\u001b[39;49msteps():\n\u001b[0;32m   2381\u001b[0m             callbacks\u001b[39m.\u001b[39;49mon_predict_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[0;32m    145\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1347\u001b[0m, in \u001b[0;36mDataHandler.catch_stop_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m-> 1347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msync()\n\u001b[0;32m   1348\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mStopIteration\u001b[39;00m, tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mOutOfRangeError):\n\u001b[0;32m   1349\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1337\u001b[0m, in \u001b[0;36mDataHandler.sync\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msync\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1337\u001b[0m     context\u001b[39m.\u001b[39;49masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:2690\u001b[0m, in \u001b[0;36masync_wait\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2688\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \u001b[39mif\u001b[39;00m context()\u001b[39m.\u001b[39m_context_handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2690\u001b[0m   context()\u001b[39m.\u001b[39;49msync_executors()\n",
      "File \u001b[1;32mc:\\Users\\guill\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:712\u001b[0m, in \u001b[0;36mContext.sync_executors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sync both local executors and the ones on remote workers.\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \n\u001b[0;32m    702\u001b[0m \u001b[39mIn async execution mode, local function calls can return before the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[39m  ValueError: if context is not initialized.\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context_handle:\n\u001b[1;32m--> 712\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39;49mTFE_ContextSyncExecutors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context_handle)\n\u001b[0;32m    713\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    714\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mContext is not initialized.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import cv2\n",
    "import FaceNormalizationUtils as faceutils\n",
    "\n",
    "def AddDecorate(frame,resizedBat,x,y):\n",
    "    \n",
    "    resizedBatMask = resizedBat[:,:,0]\n",
    "    cv2.imshow(\"2\",resized_image)\n",
    "    regionFrame = frame[y:y+resizedBat.shape[0],x:x+resizedBat.shape[1]]\n",
    "    BatMaskInv = cv2.bitwise_not(resizedBatMask)\n",
    "    BgregionFrame = cv2.bitwise_and(regionFrame,regionFrame,mask=BatMaskInv)\n",
    "    Bat = cv2.add(BgregionFrame,resizedBat)\n",
    "    frame[y:y+resizedBat.shape[0],x:x+resizedBat.shape[1]] = Bat\n",
    "\n",
    "def addPNG(frame,resized_image):\n",
    "    \n",
    "    n_frame = frame[y: y + h, x: x + w]\n",
    "    #Establecemos la mascara para el video\n",
    "    mask = resized_image[:,:,3]\n",
    "    cv2.imshow(\"1\",mask)\n",
    "\n",
    "    # Invertir la máscara\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "    bg_black = cv2.bitwise_and(resized_image, resized_image, mask=mask)\n",
    "    bg_black = bg_black[:, :, 0:3]\n",
    "    bg_frame = cv2.bitwise_and(n_frame, n_frame, mask=mask_inv)\n",
    "\n",
    "    #Resultado final y lo agregamos el frame\n",
    "    final_pumpki = cv2.add(bg_black,bg_frame) \n",
    "    \n",
    "    return final_pumpki\n",
    "# My face detectors interface\n",
    "def oscurecer_frame(frame):\n",
    "    \"\"\"\n",
    "    Oscurece un frame sin que pierda el RGB.\n",
    "\n",
    "    Args:\n",
    "        frame: El frame a oscurecer.\n",
    "\n",
    "    Returns:\n",
    "        El frame oscurecido.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Convertir el frame a escala de grises.\n",
    "    img_gris = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar un filtro de suavizado.\n",
    "    img_suavizado = cv2.GaussianBlur(img_gris, (5, 5), 0)\n",
    "\n",
    "    # Convertir el frame de escala de grises a RGB.\n",
    "    img_rgb = cv2.cvtColor(img_suavizado, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Multiplicar cada canal RGB por un factor.\n",
    "    img_rgb[:, :, 0] = img_rgb[:, :, 0] * 0.3\n",
    "    img_rgb[:, :, 1] = img_rgb[:, :, 1] * 0.3\n",
    "    img_rgb[:, :, 2] = img_rgb[:, :, 2] * 0.3\n",
    "\n",
    "    return img_rgb\n",
    "    \n",
    "def rotate_image(image, angle):\n",
    "    center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return rotated_image\n",
    "\n",
    "FDet = FaceDetectors.FaceDetector()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Carga las imágenes de máscaras\n",
    "mask_images = []\n",
    "mask_images.append(cv2.imread(\"spookyPumpki2.png\",cv2.IMREAD_UNCHANGED))\n",
    "mask_images.append(cv2.imread(\"esqueleto.png\",cv2.IMREAD_UNCHANGED))\n",
    "mask_images.append(cv2.imread(\"scream.png\",cv2.IMREAD_UNCHANGED))\n",
    "\n",
    "current_mask_index = 0  # Índice de la máscara actual\n",
    "\n",
    "#print('pumpkin.shape =',pumpkin.shape)\n",
    "#cv2.imshow('pumpkin',pumpkin[:,:,3])\n",
    "faceClassif = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Check for other cameras\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print('Camera error')\n",
    "            exit(0)\n",
    "        else:\n",
    "            print('Camera 0')\n",
    "    else:\n",
    "        print('Camera 1')\n",
    "else:\n",
    "    print('Camera 0')\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "  ret, frame = cap.read()\n",
    "  frame = oscurecer_frame(frame)\n",
    "\n",
    "  if ret == False:\n",
    "    break\n",
    "  \n",
    "  faces = faceClassif.detectMultiScale(frame,1.3,5)\n",
    "\n",
    "  values = FDet.SingleFaceEyesDetection(frame, FDet.FaceDetectors[0], FDet.EyeDetectors[1])\n",
    "\n",
    "    # Detectamos las caras en el fotograma.\n",
    "  if values is not None:\n",
    "    face, eyes, shape = values\n",
    "\n",
    "  [x, y , w, h] = face\n",
    "\n",
    "  if x > -1:\n",
    "    #Redimensionar la imagen \n",
    "\n",
    "    resized_image = cv2.resize(mask_images[current_mask_index], (w, h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    #Aplicamos filtros para oscurecer la imagen\n",
    "    \n",
    "    imgOnFrame = addPNG(frame,resized_image)\n",
    "\n",
    "  # Encuentra los landmarks faciales.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    # Encuentra los puntos clave de los ojos.\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "\n",
    "   # Calcula el ángulo de inclinación de la cabeza.\n",
    "    left_eye_center = left_eye.mean(axis=0).astype(\"int\")\n",
    "    right_eye_center = right_eye.mean(axis=0).astype(\"int\")\n",
    "    dY = left_eye_center[1] - right_eye_center[1]\n",
    "    dX = left_eye_center[0] - right_eye_center[0]\n",
    "\n",
    "    # Calcula el ángulo de inclinación de la cabeza en función de la posición de los ojos.\n",
    "    if right_eye[0][0] > left_eye[0][0]:\n",
    "        # La cabeza está inclinada hacia la izquierda, rota en sentido contrario.\n",
    "        angle = -np.degrees(np.arctan2(dY, dX))\n",
    "    else:\n",
    "        # La cabeza está inclinada hacia la derecha, rota en sentido normal.\n",
    "        angle = np.degrees(np.arctan2(dY, dX))\n",
    "\n",
    "\n",
    "    # Rota la máscara según el ángulo de la cabeza utilizando tu función\n",
    "    rotated_mask = rotate_image(imgOnFrame, angle)\n",
    "\n",
    "    # Aplica una rotación adicional de 180 grados a la máscara para corregir la orientación\n",
    "    rotated_mask = rotate_image(rotated_mask, 180)\n",
    "\n",
    "\n",
    "    # Aplica la máscara rotada al frame\n",
    "    frame[y: y + h, x: x + w] = rotated_mask\n",
    "\n",
    "  resizedBat= cv2.resize(cv2.imread(\"bat.png\"), (364//3,364//3), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "  AddDecorate(frame,resizedBat,10,10)\n",
    "  AddDecorate(frame,resizedBat,500,30)\n",
    "\n",
    "\n",
    "  cv2.imshow('Halloween',frame)\n",
    "\n",
    "  key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "  if key == 27:\n",
    "    break\n",
    "  elif key == 32:  # Barra espaciadora\n",
    "        # Cambia a la siguiente máscara\n",
    "        current_mask_index = (current_mask_index + 1) % len(mask_images)\n",
    "\n",
    "\n",
    "# Liberamos los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('FACES')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea3a1ee99ce326e593ddb52cd278556d527fcb6552c40e2a47b1efb9d0183637"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
